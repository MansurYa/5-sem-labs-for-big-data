# 5-sem-labs-for-big-data

В этом репозитории хранится весь код, написанный в течении прохождения курса "Большие данные" в 5-м семестре факультета ПМ-ПУ, университета СПбГУ.

## Проекты

### map_reduce

**Описание:** Реализация MapReduce для вычисления средней цены по категориям товаров с сравнением производительности трех подходов: базовый последовательный (pandas groupby), последовательный MapReduce и параллельный MapReduce.

**Задача:** Посчитать среднюю цену в каждой категории товаров из e-commerce данных.

**Алгоритм MapReduce:**
- **Map:** Для каждого chunk вычисляется сумма цен и количество товаров по категориям
- **Reduce:** Объединение результатов путем суммирования сумм и количеств
- **Finalize:** Вычисление средней цены как `sum / count`

**Запуск:**
```bash
cd map_reduce
python map_reduce.py
```

**Результаты бенчмарков:**

**Эксперимент 1: Средний объем данных**

Тестирование на датасете `data/kz.csv` (2,633,521 строк, 900 категорий):

| Подход | Среднее время (10 запусков) | Относительная производительность |
|--------|------------------------------|----------------------------------|
| Базовый последовательный (pandas groupby) | 0.0903 сек | 1.0x (baseline) |
| Последовательный MapReduce | 0.1450 сек | 0.62x (медленнее в 1.61x) |
| Параллельный MapReduce (8 процессов) | 1.0372 сек | 0.09x (медленнее в 11.5x) |

**Эксперимент 2: Большой объем данных**

Тестирование на синтетическом датасете (500,000,000 строк, 500 категорий):

| Подход | Среднее время (2 запуска) | Относительная производительность |
|--------|---------------------------|----------------------------------|
| Базовый последовательный (pandas groupby) | 99.86 сек | 1.0x (baseline) |
| Последовательный MapReduce | 38.31 сек | **2.61x (быстрее!)** |
| Параллельный MapReduce (8 процессов) | 56.86 сек | **1.76x (быстрее!)** |

**Выводы:**
1. На средних объемах (~2.6M строк) pandas groupby эффективнее из-за overhead на создание процессов
2. На больших объемах (~500M строк) MapReduce подход показывает **значительное преимущество**:
   - Sequential MapReduce быстрее в **2.6 раза**
   - Parallel MapReduce быстрее в **1.76 раза**
3. Sequential MapReduce оказался быстрее Parallel из-за overhead на межпроцессное взаимодействие при передаче больших объемов данных
4. MapReduce эффективен на действительно больших данных благодаря chunk-based обработке и оптимизации агрегации

---

## ozone_orders

### Описание задачи

Есть заказы Ozon, которые распредлены по полигонам. Для всех заказов задана матрица расстояний, которая содержит _время_ перемещения между заказами.

### Данные

- [Cписок заказов OZON](https://disk.yandex.ru/d/3C8SSe3yKEg7wA)
- [Матрица_расстояний_между_заказами](https://disk.yandex.ru/d/oGCeE4Yk7hNMHA)

### Описание файлов

В 1м файле представлены записи вида:
{"ID":20660,"Lat":55.661591,"Long":37.416988,"MpId":9182},где:  ID - уникальный код заказа (или пункта выдачи), Lat/Long - географические коордиинаты заказа, MpId - код полигона к которому относится заказ

Во 2-м файле содержится матрица расстояний между всеми заказами. Это массив записей вида:
,{"from":20660,"to":24274,"dist":56}где: from - ID заказа (старт), to - ID заказа (финиш), dist - время перемещения (в секундах)

Важно: несмотря на то, что заданы географические координаты заказов, для бизнеса ключевое значение имеет именно матрица расстояний. т.к. она показывает за какое время происходит перемещение между ними (с учетом правил движения и пр.)

### Задача

1) Используя первый файл, необходимо отобразить все заказы на карте и отразить их привязку к полигонам
2) Используя файл с матрицей расстояний, необхъодимо объединить все заказы в группы (например для дальнейшей передачи курьеру) так, чтобы заказы, относящиеся к одному полигону, не оказались в разных группах

---

## hadoop/git_analysis

**Описание:** Hadoop MapReduce приложение для анализа вклада компаний в Git репозитории на основе email адресов авторов коммитов.

**Структура:**
```
hadoop/git_analysis/
├── src/
│   ├── mapper.py         # Извлекает домен компании из email
│   └── reducer.py        # Суммирует коммиты по компаниям
├── prepare_data.py       # Извлечение email из Git репозитория
└── run_hadoop.sh         # Запуск MapReduce на кластере
```

### Первый запуск

**1. Запуск Hadoop кластера:**
```bash
cd hadoop/hadoop-cluster-docker
docker-compose up -d
```

Подождите 20-30 секунд для инициализации всех сервисов.

**2. Подготовка данных:**
```bash
# Клонируйте анализируемый репозиторий
git clone https://github.com/username/repo.git /path/to/repo

# Извлеките email авторов
cd hadoop/git_analysis
python prepare_data.py /path/to/repo git_commits.txt
```

**3. Копирование файлов в контейнер:**
```bash
docker cp git_analysis hadoop-cluster-docker-namenode-1:/opt/
docker cp git_commits.txt hadoop-cluster-docker-namenode-1:/opt/git_analysis/
```

**4. Запуск анализа:**
```bash
docker exec -it hadoop-cluster-docker-namenode-1 bash
cd /opt/git_analysis
bash run_hadoop.sh git_commits.txt output
```

Результаты выведутся автоматически после завершения, отсортированные по количеству коммитов.

### Анализ другого репозитория

Если кластер уже запущен и вы хотите проанализировать новый репозиторий:

```bash
# 1. Подготовьте данные локально
cd hadoop/git_analysis
python prepare_data.py /path/to/new-repo git_commits_new.txt

# 2. Скопируйте в контейнер
docker cp git_commits_new.txt hadoop-cluster-docker-namenode-1:/opt/git_analysis/

# 3. Запустите анализ с новым именем выходной директории
docker exec -it hadoop-cluster-docker-namenode-1 bash
cd /opt/git_analysis
bash run_hadoop.sh git_commits_new.txt output_new

# 4. Сохранить данные
hdfs dfs -get /user/output/part-00000 /tmp/results_new.txt
docker cp hadoop-cluster-docker-namenode-1:/tmp/results_new.txt /Users/mansurzainullin/MyCode/5-sem-labs-for-big-data/data/
```

### Решение проблем

**Проблема: DataNode не запускается (ошибка "0 datanode(s) running")**

Причина: Несовпадение clusterID после перезапуска контейнеров.

Решение:
```bash
cd hadoop/hadoop-cluster-docker
docker-compose down
docker volume prune -f
docker-compose up -d
```

После этого заново скопируйте файлы (шаг 3 из "Первый запуск").

**Проблема: Задача зависла на "Running job"**

Проверьте статус NodeManager:
```bash
docker exec hadoop-cluster-docker-resourcemanager-1 yarn node -list
```

Должна быть 1 активная нода. Если нет - выполните полный перезапуск кластера (см. выше).

### Формат результатов

```
users.noreply.github.com    3358
individual                   311
company.com                   66
```

- `individual` — коммиты с личных email (gmail.com, yandex.ru и т.д.)
- `unknown` — некорректные email адреса
- остальные — домены компаний
